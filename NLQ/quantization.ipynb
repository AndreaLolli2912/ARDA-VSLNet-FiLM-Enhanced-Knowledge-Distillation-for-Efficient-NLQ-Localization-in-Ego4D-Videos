{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3baf1825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\Documents\\University\\1st Year\\Machine Learning and Deep Learning\\Ego4D-NLQ\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.ao.quantization import default_qconfig\n",
    "from torch.ao.quantization.fuse_modules import fuse_modules\n",
    "from torch.ao.quantization.qconfig import float_qparams_weight_only_qconfig\n",
    "from model.layers import Embedding\n",
    "\n",
    "from model.DeepVSLNet import DeepVSLNet\n",
    "from model.QuantizedDeepVSLNet import QuantizedDeepVSLNet\n",
    "from model.layers import Conv1DReLU, Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6241e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_sequential_block(\n",
    "    block: nn.Sequential,\n",
    "    layers_to_fuse: list[str],\n",
    "    inplace: bool = False\n",
    ") -> nn.Sequential:\n",
    "    \"\"\"\n",
    "    Fuse specified layers in a Sequential block.\n",
    "    \"\"\"\n",
    "    return torch.quantization.fuse_modules(block, layers_to_fuse, inplace=inplace)\n",
    "\n",
    "def fuse_modulelist_blocks(\n",
    "    blocks: nn.ModuleList,\n",
    "    fuse_map: list[str],\n",
    "    inplace: bool = False\n",
    ") -> nn.ModuleList:\n",
    "    \"\"\"\n",
    "    Applies fusion to each nn.Sequential in a ModuleList using a fixed fuse pattern.\n",
    "    \"\"\"\n",
    "    fused = []\n",
    "    for block in blocks:\n",
    "        block_copy = block if inplace else copy.deepcopy(block)\n",
    "        fused_block = fuse_sequential_block(block_copy, fuse_map, inplace=inplace)\n",
    "        fused.append(fused_block)\n",
    "    return nn.ModuleList(fused)\n",
    "\n",
    "def fuse_depthwise_separable_conv_block(conv_block, inplace=False):\n",
    "    \"\"\"\n",
    "    Fuses the pointwise Conv1d + ReLU in a DepthwiseSeparableConvBlock.\n",
    "    \"\"\"\n",
    "    conv_block_copy = conv_block if inplace else copy.deepcopy(conv_block)\n",
    "    fuse_pattern = ['1', '2']  # pointwise conv + ReLU\n",
    "    conv_block_copy.depthwise_separable_conv = fuse_modulelist_blocks(\n",
    "        conv_block_copy.depthwise_separable_conv,\n",
    "        fuse_map=fuse_pattern,\n",
    "        inplace=inplace\n",
    "    )\n",
    "    return conv_block_copy\n",
    "\n",
    "def fuse_feature_encoder(feature_encoder, inplace=False):\n",
    "    \"\"\"\n",
    "    Fuses all submodules in the feature encoder.\n",
    "    \"\"\"\n",
    "    encoder = feature_encoder if inplace else copy.deepcopy(feature_encoder)\n",
    "    encoder.conv_block = fuse_depthwise_separable_conv_block(encoder.conv_block, inplace=inplace)\n",
    "    return encoder\n",
    "\n",
    "def fuse_conv1d_relu_in_sequential(seq: nn.Sequential) -> nn.Sequential:\n",
    "    layers = []\n",
    "    i = 0\n",
    "    while i < len(seq):\n",
    "        if (\n",
    "            isinstance(seq[i], Conv1D)\n",
    "            and i + 1 < len(seq)\n",
    "            and isinstance(seq[i + 1], nn.ReLU)\n",
    "        ):\n",
    "            fused = Conv1DReLU(seq[i])\n",
    "            layers.append(fused)\n",
    "            i += 2  # skip next\n",
    "        else:\n",
    "            layers.append(seq[i])\n",
    "            i += 1\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def fuse_predictor_head(predictor_head: nn.Sequential, inplace=False) -> nn.Sequential:\n",
    "    block = predictor_head if inplace else copy.deepcopy(predictor_head)\n",
    "    return fuse_conv1d_relu_in_sequential(block)\n",
    "\n",
    "def fuse_conditioned_predictor(conditioned_predictor, inplace=False):\n",
    "    \"\"\"\n",
    "    Fuses encoder and start/end heads in a conditioned predictor module.\n",
    "    \"\"\"\n",
    "    predictor = conditioned_predictor if inplace else copy.deepcopy(conditioned_predictor)\n",
    "    predictor.encoder = fuse_feature_encoder(predictor.encoder, inplace=inplace)\n",
    "    predictor.start_block = fuse_predictor_head(predictor.start_block, inplace=inplace)\n",
    "    predictor.end_block = fuse_predictor_head(predictor.end_block, inplace=inplace)\n",
    "    return predictor\n",
    "\n",
    "def fuse_model(model, inplace=False):\n",
    "    \"\"\"\n",
    "    Top-level model fusion function.\n",
    "    \"\"\"\n",
    "    model_copy = model if inplace else copy.deepcopy(model)\n",
    "    model_copy.feature_encoder = fuse_feature_encoder(model_copy.feature_encoder, inplace=inplace)\n",
    "    model_copy.predictor = fuse_conditioned_predictor(model_copy.predictor, inplace=inplace)\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220f0c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Namespace(\n",
    "    video_feature_dim=256,\n",
    "    dim=256,\n",
    "    film_mode=\"inside_encoder:multi\",\n",
    "    drop_rate=0,\n",
    "    word_size=300,\n",
    "    char_size=1000,\n",
    "    word_dim=300,\n",
    "    char_dim=50,\n",
    "    word_vectors=None,\n",
    "    num_heads=8,\n",
    "    max_pos_len=128,\n",
    "    predictor=\"glove\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a160a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.qconfig import QConfig\n",
    "from torch.ao.quantization.observer import MinMaxObserver\n",
    "from torch.ao.quantization.observer import default_observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f29bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_model = QuantizedDeepVSLNet(configs=configs, word_vectors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642664ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pop(\"linear_modulation.film_generator.weight\", None)\n",
    "a.pop(\"linear_modulation.film_generator.bias\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d02d3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'linear_modulation.film_generator.bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlinear_modulation.film_generator.bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'linear_modulation.film_generator.bias'"
     ]
    }
   ],
   "source": [
    "a[\"linear_modulation.film_generator.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4293d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"deepvslnet_11649.t7\", map_location='cpu')\n",
    "# a.remove(\"linear_modulation.film_generator.weight\", \"linear_modulation.film_generator.bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load your original model\n",
    "float_model = DeepVSLNet(configs=configs, word_vectors=None)\n",
    "float_model.eval()\n",
    "\n",
    "fused_model = fuse_model(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c057b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig_global = QConfig(\n",
    "    activation=MinMaxObserver.with_args(dtype=torch.qint8),\n",
    "    weight=default_observer.with_args(dtype=torch.qint8)\n",
    ")\n",
    "\n",
    "qconfig_emb = float_qparams_weight_only_qconfig\n",
    "\n",
    "\n",
    "def assign_qconfig(model, qconfig_global, qconfig_emb):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Embedding):\n",
    "            module.qconfig = None\n",
    "            print(name, module)\n",
    "        else:\n",
    "            # For other modules, assign global only if they don't have qconfig yet\n",
    "            if not hasattr(module, 'qconfig') or module.qconfig is None:\n",
    "                module.qconfig = qconfig_global\n",
    "\n",
    "assign_qconfig(fused_model, qconfig_global, qconfig_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e85d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: insert observers\n",
    "quant_ready_model = QuantizedDeepVSLNet(fused_model)\n",
    "\n",
    "torch.ao.quantization.prepare(quant_ready_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84cc4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: calibration\n",
    "# run_static_quantization_calibration(\n",
    "#     quant_ready_model, calibration_loader, num_calibration_batches\n",
    "# )\n",
    "\n",
    "# 4: convert to quantized\n",
    "quantized_model = torch.ao.quantization.convert(quant_ready_model, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ddbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(quantized_model.state_dict(), 'quantized_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29297d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('quantized_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143efedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"video_affine.linear.conv1d.weight\"].dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ego4D-NLQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
